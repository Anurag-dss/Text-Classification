# -*- coding: utf-8 -*-
"""Text Classification NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lhxL5nmR69D58A3YVwG8znp8mhuZr3LT
"""

import tensorflow as tf
import tensorflow_datasets as tfds
import matplotlib.pyplot as plt

(train_data, test_data), info = tfds.load(
    "imdb_reviews",
    split=["train", "test"],
    as_supervised=True,
    with_info=True
)

print(info)

BUFFER_SIZE = 10000
BATCH_SIZE = 32

train_data = train_data.shuffle(BUFFER_SIZE)
test_data = test_data.batch(BATCH_SIZE)

from tensorflow.keras.layers import TextVectorization

vectorizer = TextVectorization(
    max_tokens=10000,
    output_sequence_length=250
)

text_only_train = train_data.map(lambda x, y: x)
vectorizer.adapt(text_only_train)

train_data = train_data.batch(BATCH_SIZE).map(
    lambda x, y: (vectorizer(x), y)
)

test_data = test_data.map(
    lambda x, y: (vectorizer(x), y)
)

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=10000, output_dim=16),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(16, activation="relu"),
    tf.keras.layers.Dense(1, activation="sigmoid")
])

model.compile(
    optimizer="adam",
    loss="binary_crossentropy",
    metrics=["accuracy"]
)

model.summary()

history = model.fit(
    train_data,
    epochs=10,
    validation_data=test_data
)

model.evaluate(test_data)

import matplotlib.pyplot as plt

plt.plot(history.history["accuracy"], label="Train Accuracy")
plt.plot(history.history["val_accuracy"], label="Validation Accuracy")
plt.legend()
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.show()

plt.plot(history.history["loss"], label="Train Loss")
plt.plot(history.history["val_loss"], label="Validation Loss")
plt.legend()
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.show()

model.predict(vectorizer(["this movie was amazing and inspiring"]))

import numpy as np
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

y_true = []
y_pred = []

for x, y in test_data:
    preds = model.predict(x)
    y_true.extend(y.numpy())
    y_pred.extend((preds > 0.5).astype(int))

cm = confusion_matrix(y_true, y_pred)
disp = ConfusionMatrixDisplay(cm, display_labels=["Negative", "Positive"])
disp.plot()

pred_probs = []

for x, y in test_data:
    preds = model.predict(x)
    pred_probs.extend(preds.flatten())

plt.figure(figsize=(8,5))
plt.hist(pred_probs, bins=50)
plt.title("Prediction Probability Distribution")
plt.xlabel("Predicted Probability")
plt.ylabel("Frequency")
plt.show()

texts = []
true_labels = []
pred_labels = []

for x, y in test_data.take(1):
    preds = model.predict(x)
    texts.extend(x.numpy())
    true_labels.extend(y.numpy())
    pred_labels.extend((preds > 0.5).astype(int))

for i in range(5):
    print("Review:", texts[i][:200])
    print("Actual:", "Positive" if true_labels[i] == 1 else "Negative")
    print("Predicted:", "Positive" if pred_labels[i] == 1 else "Negative")
    print("-" * 80)

plt.figure(figsize=(5,4))
plt.bar(["Test Accuracy"], [0.8525])
plt.ylim(0,1)
plt.title("Final Model Accuracy")
plt.ylabel("Accuracy")
plt.show()

from tensorflow.keras.datasets import imdb

# Load word index
word_index = imdb.get_word_index()

# Reserve special tokens
word_index = {k: (v + 3) for k, v in word_index.items()}
word_index["<PAD>"] = 0
word_index["<START>"] = 1
word_index["<UNK>"] = 2
word_index["<UNUSED>"] = 3

reverse_word_index = {value: key for key, value in word_index.items()}

def decode_review(encoded_review):
    return " ".join(
        reverse_word_index.get(i, "?") for i in encoded_review
    )

for x, y in test_data.take(1):
    preds = model.predict(x)

    for i in range(5):
        print("REVIEW:")
        print(decode_review(x[i].numpy())[:700], "...\n")
        print("Actual:", "Positive" if y[i].numpy() == 1 else "Negative")
        print("Predicted:", "Positive" if preds[i] > 0.5 else "Negative")
        print("=" * 100)

review_lengths = []

for x, y in train_data.take(200):
    for review in x.numpy():
        review_lengths.append(len(review))

import matplotlib.pyplot as plt

plt.figure(figsize=(8,5))
plt.hist(review_lengths, bins=40)
plt.title("Distribution of Review Lengths")
plt.xlabel("Number of Words")
plt.ylabel("Frequency")
plt.show()

